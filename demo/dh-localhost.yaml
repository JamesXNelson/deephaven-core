# This is a bit ugly, but, for now, we're going to define the entire cluster in this one file.
# This includes pulling out all the envoy config, so we can change things as we please
# without interfering with the simple docker-compose deployment using envoy.yaml in source tree
apiVersion: v1
kind: ConfigMap
metadata:
  name: envoy-conf
data:
  envoy.yaml: |
    admin:
      # access_log_path: /dev/stdout
      access_log_path: /tmp/admin_access.log
      address:
        socket_address:
          address: 127.0.0.1
          port_value: 9090
    static_resources:
      listeners:
      - name: listener_0
        address:
          socket_address:
            address: 0.0.0.0
            port_value: 10000
        filter_chains:
        - filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              access_log:
              - name: envoy.access_loggers.stdout
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog
              codec_type: AUTO
              stat_prefix: ingress_https
              upgrade_configs:
               - upgrade_type: websocket
              route_config:
                name: local_route
                virtual_hosts:
                    - name: reverse_proxy
                      domains: ["*"]
                      routes:
                        - match: # Call to / goes to the landing page
                            path: "/"
                          route: { cluster: web }
                        - match: # Web IDE lives in this path
                            prefix: "/ide"
                          route: { cluster: web }
                        - match: # JS API lives in this path
                            prefix: "/jsapi"
                          route: { cluster: web }
                        - match: # Notebook file storage at this path
                            prefix: "/notebooks"
                          route: { cluster: web }
                        - match: # Any GRPC call is assumed to be forwarded to the real service
                            prefix: "/"
                            grpc: {}
                          route:
                            cluster: grpc-api
                            max_stream_duration:
                              grpc_timeout_header_max: 0s
                            timeout: 0s
                        - match: # Any other call made will be forwarded to the grpc websocket proxy
                            prefix: "/"
                          route:
                            cluster: grpc-proxy
                http_filters:
                  - name: envoy.filters.http.health_check
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.http.health_check.v3.HealthCheck
                      pass_through_mode: false
                      headers:
                        - name: ":path"
                          exact_match: "/healthz"
                        - name: "x-envoy-livenessprobe"
                          exact_match: "healthz"
                  - name: envoy.filters.http.grpc_web
                  - name: envoy.filters.http.router
                clusters:
                  - name: grpc-proxy
                    connect_timeout: 10s
                    type: LOGICAL_DNS
                    lb_policy: ROUND_ROBIN
                    load_assignment:
                      cluster_name: grpc-proxy
                      endpoints:
                        - lb_endpoints:
                            - endpoint:
                                address:
                                  socket_address:
                                    # address: grpc-proxy # assuming the name of the grpc-api server container
                                    address: 127.0.0.1
                                    port_value: 8008
                      # Health check requires http2
                  #      health_checks:
                  #        timeout: 1s
                  #        interval: 10s
                  #        unhealthy_threshold: 2
                  #        healthy_threshold: 2
                  #        grpc_health_check: { }
                  - name: grpc-api
                    connect_timeout: 10s
                    type: LOGICAL_DNS
                    lb_policy: ROUND_ROBIN
                    http2_protocol_options: {}
                    load_assignment:
                      cluster_name: grpc-proxy
                      endpoints:
                        - lb_endpoints:
                            - endpoint:
                                address:
                                  socket_address:
                                    # address: grpc-api # here we assume the name of the websocket proxy
                                    address: 127.0.0.1
                                    port_value: 8080
                  #      health_checks:
                  #        timeout: 1s
                  #        interval: 10s
                  #        unhealthy_threshold: 2
                  #        healthy_threshold: 2
                  #        grpc_health_check: { }
                  - name: web
                    connect_timeout: 10s
                    type: LOGICAL_DNS
                    lb_policy: ROUND_ROBIN
                    http_protocol_options: {}
                    load_assignment:
                      cluster_name: web
                      endpoints:
                        - lb_endpoints:
                            - endpoint:
                                hostname: web
                                address:
                                  socket_address:
                                    # address: web
                                    address: 127.0.0.1
                                    port_value: 80

          transport_socket:
            name: envoy.transport_sockets.tls
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext
              require_client_certificate: false
              common_tls_context:
                tls_certificates:
                - certificate_chain:
                    filename: /etc/ssl/envoy/tls.crt
                  private_key:
                    filename: /etc/ssl/envoy/tls.key
---
kind: DaemonSet
apiVersion: apps/v1
metadata:
  labels:
    app: envoy
  name: envoy
spec:
  selector:
    matchLabels:
      app: envoy
  template:
    metadata:
      labels:
        app: envoy
      name: envoy
    spec:
      hostIPC: true
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
        - name: envoy
          image: envoyproxy/envoy-dev:a1c5a5076e97e6cd3d4b725b6eeb645513c72092
          command: ["/usr/local/bin/envoy"]
          args:
            - "--concurrency 4"
            - "--config-path /etc/envoy/envoy.json"
            - "--mode serve"
          ports:
            - containerPort: 10000
              protocol: TCP
            - containerPort: 9090
              protocol: TCP
          resources:
            limits:
              cpu: "1000m"
              memory: "512Mi"
            requests:
              cpu: "100m"
              memory: "64Mi"
          volumeMounts:
            - name: envoy-conf
              mountPath: /etc/envoy
      volumes:
        - name: envoy-conf
          configMap:
            name: envoy
---
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: kubernetes-envoy-sds
  name: kubernetes-envoy-sds
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubernetes-envoy-sds
  template:
    metadata:
      labels:
        app: kubernetes-envoy-sds
      name: kubernetes-envoy-sds
    spec:
      containers:
        - name: kubernetes-envoy-sds
          image: gcr.io/hightowerlabs/kubernetes-envoy-sds:0.0.1
          imagePullPolicy: Always
          args:
            - "-http=0.0.0.0:10000"
        - name: kubectl
          image: gcr.io/google_containers/hyperkube:v1.7.2
          command:
            - "/hyperkube"
          args:
            - "kubectl"
            - "proxy"
---
apiVersion: v1
kind: Service
metadata:
  name: dh-local
  labels:
    run: dh-local
spec:
  # NodePort allows you to expose a port on, say, localhost minikube ip: `minikube service --url dh-local`
  type: NodePort
  # If you want to run localhost with tls, you may use one of two ways,
  # assuming tls_port=8443
  # 1) create a self signed certificate using any DNS name you would like, and accept security warnings in browser:
  #    TODO: finish these instructions
  # 2) use an existing cert for your domain.name
  #    If you do not want to hack /etc/hosts or resolv.conf, pick a domain name and create a DNS A record pointing to 127.0.0.1
  #    If using DNS to 127.0.0.1, port-forward from minikube to localhost:
  #    kubectl port-forward pods/dh-local-6674d78b6-cc64l ${tls_port:-8443}:10000
  #    Next, supply your certificates (directory is .gitignore'd)
  #    cp tls.crt tls.key $deephaven_core_directory/demo/certs/
  #    Then, forcibly restart your local cluster
  #    { k delete deployment dh-local || true ; } && k apply -f ./pod-dh.yaml
  #    Now, visit https://domain.name:8433 and rejoice.
  ports:
    -
      name: envoy-client
      port: 10000
      targetPort: 10000
      protocol: TCP
      nodePort: 30080
    -
      name: envoy-admin
      port: 9090
      targetPort: 9090
      protocol: TCP
      nodePort: 30443
  selector:
    # This service runs a deployment. That deployment is dh-local, below:
    run: dh-local
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dh-local
spec:
  selector:
    matchLabels:
      run: dh-local
  replicas: 1
  template:
    metadata:
      labels:
        run: dh-local
    spec:
      volumes:
        - name: secret-volume
          secret:
            secretName: nginxsecret
        - name: configmap-volume
          configMap:
            name: nginxconfigmap
        - name: cred-cache
          emptyDir:
            medium: Memory

      containers:

        #
        # grpc-api container.  Where all the stateful brains live
        #
        - name: grpc-api
          image: deephaven/grpc-api:local-build
          imagePullPolicy: Never
          securityContext:
            privileged: false
          # Change tty to true if you want to be able to shell into the box
          tty: true
          env:
          - name: CONTAINER_NAME
            value: grpc-api
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: MY_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                fieldPath: spec.serviceAccountName
          volumeMounts:
          - name: cred-cache
            mountPath: /root/.ssh
          readinessProbe:
            exec:
              command: [ "/health/grpc_health_probe", "-addr=:8080" ]
            initialDelaySeconds: 2
            periodSeconds: 1
            failureThreshold: 28
          livenessProbe:
            exec:
              command: [ "/health/grpc_health_probe", "-addr=:8080" ]
            initialDelaySeconds: 10
            periodSeconds: 8

        #
        # grpc-proxy: turns http1 grpc web requests into http2 fit for grpc-api to consume
        #
        - name: grpc-proxy
          image: deephaven/grpc-proxy:local-build
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
          # Change tty to true if you want to attach to the image entry point.
          # you can still launch a tty shell with `kubectl -c grpc-proxy -it MY_POD_NAME_HERE -- bash`
          # You can find MY_POD_NAME_HERE from `minikube dashboard` or, to find the newest pod containing name dh-local:
          # fmt="jsonpath={range .items[*]}{.status.startTime}{'\t'}{.metadata.name}{'\n'}{end}"
          # my_pod="$(kubectl get pods --sort-by=.metadata.creationTimestamp --no-headers -o="$fmt" | grep dh-local | tail -n 1 | awk '{print $2}')"
          # kubectl -c grpc-proxy -it $my_pod -- sh
          tty: false
      #    resources:
      #      # See: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md
      #      limits:
      #        # 4 cpus
      #        cpu: '4000m'
      #        # ~8G RAM
      #        memory: '8000Mi'
          env:
          - name: BACKEND_ADDR
            value: "localhost:8080"
          - name: CONTAINER_NAME
            value: grpc-proxy
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                fieldPath: spec.serviceAccountName
          volumeMounts:
          - name: cred-cache
            mountPath: /root/.ssh


        #
        # web: serves the web ide and all static resources
        #
        - name: web
          image: deephaven/web:local-build
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
          # Change tty to true if you want to be able to shell into the box
          tty: false
          env:
          - name: CONTAINER_NAME
            value: web
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                fieldPath: spec.serviceAccountName
          volumeMounts:
          - name: cred-cache
            mountPath: /root/.ssh


        #
        # envoy: this should really be a stateful set / deployment instead of a container-within-this-pod.
        # As a stateful set, the container would be added automatically to all nodes, plus it would have stable addressing.
        #
#        - name: envoy
#          image: deephaven/envoy:local-build
#          imagePullPolicy: IfNotPresent
#          securityContext:
#            privileged: false
#          # Change tty to true if you want to be able to attach directly to the envoy process
#          tty: false
#          ports:
#            - name: https
#              containerPort: 8443
#            - name: http
#              containerPort: 8181
#              protocol: TCP
#              # haven't figured out readiness probe yet
##          readinessProbe:
##            httpGet:
##              port: https
##              httpHeaders:
##                - name: x-envoy-livenessprobe
##                  value: healthz
##              path: /healthz
##              scheme: HTTPS
#          env:
#          - name: CONTAINER_NAME
#            value: envoy
#          - name: HOST_IP
#            valueFrom:
#              fieldRef:
#                fieldPath: status.hostIP
#          - name: MY_NODE_NAME
#            valueFrom:
#              fieldRef:
#                fieldPath: spec.nodeName
#          - name: MY_POD_NAME
#            valueFrom:
#              fieldRef:
#                fieldPath: metadata.name
#          - name: MY_POD_NAMESPACE
#            valueFrom:
#              fieldRef:
#                fieldPath: metadata.namespace
#          - name: MY_POD_IP
#            valueFrom:
#              fieldRef:
#                fieldPath: status.podIP
#          - name: MY_POD_SERVICE_ACCOUNT
#            valueFrom:
#              fieldRef:
#                fieldPath: spec.serviceAccountName
#          volumeMounts:
#          - name: cred-cache
#            mountPath: /root/.ssh
